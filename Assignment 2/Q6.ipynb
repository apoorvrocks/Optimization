{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85119f0f-3d4a-4492-9269-88a83d6d4970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iters = 55 , function value = -59516.999985 for subpart 3\n",
      "iters = 31 , function value = [7.    1.001 1.    1.    1.    1.    1.    1.    1.    1.   ],for beta = 1/200\n",
      "iters = 10, function value = [7.    1.001 1.    1.    1.    1.    1.    1.    1.    1.   ] for beta = 1/700\n",
      "iters = 20 function value = [7. 1. 1. 1. 1. 1. 1. 1. 1. 1.] for beta = 1/2000\n",
      "the best beta = 1/700 and corresponding function val is [7.    1.001 1.    1.    1.    1.    1.    1.    1.    1.   ] and the point to which it converges is -59516.999998\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import re\n",
    "\n",
    "# Below function is used to get function value,gradient and hessian at a given point\n",
    "# Input: point at which function value needs to be obtained\n",
    "# Output: returns function value, gradient and Q*x0 \n",
    "def readPoint(SR, currPoint):\n",
    "    input = str(SR) + \",[\" + ','.join(str(elem) for elem in currPoint) + \"]\"\n",
    "    someVar = subprocess.run([\"Q6_oracle_2.exe\", input], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    func_val = re.search(r'(.*)', someVar).group(1)\n",
    "    func_val = float(func_val)\n",
    "    gr = re.findall(r'\\[(.*?)\\]',someVar)\n",
    "    gr = str(gr)\n",
    "    gr = re.sub('[\\'\\[\\]]', '', gr)\n",
    "    Arr1 = gr.split(',')\n",
    "    FloatArr1 = [float(i) for i in Arr1]\n",
    "    gradient = np.array(FloatArr1[0:10],dtype = np.float64)\n",
    "    Q_x = np.array(FloatArr1[10:],dtype = np.float64)\n",
    "    return func_val,gradient,Q_x\n",
    "\n",
    "## performs exact line search\n",
    "## Input: x0, flag =1 for 6 f)iii and flag = 2 for 6f)iv . When flag = 2 S_ST can take 3 values corresponding to different beta\n",
    "## Output: Returns best step size\n",
    "def exactLineSearch(x0,flag,S_ST):\n",
    "    \n",
    "    grad = readPoint(19598,x0)[1] \n",
    "    Q_grad = readPoint(19598,grad)[2] ## Returns operation of Q on grad\n",
    "    Q_grad_mod = readPoint(19598,np.dot(S_ST,grad))[2] ## Returns operation of Q on np.dot(S_ST,grad)\n",
    "    if flag==1:\n",
    "        step_size = (np.linalg.norm(grad))**2/(np.dot(grad.T,Q_grad)) ## corresponds to normal gradient descent with exact line search\n",
    "    else:\n",
    "        v = np.dot(S_ST,grad)\n",
    "        step_size = np.dot(grad.T,v)/np.dot(v.T,Q_grad_mod) ## corresponds to scaled gradient descent with exact line search\n",
    "    return step_size\n",
    "\n",
    "# Below function performs the actual gradient descent\n",
    "# Input: SR Number and the initial point \n",
    "# Output: Number of iterations required for the Gradient Descent to Converge,point after convergence and the function value \n",
    "def gradientDescent(SR, x0,flag,beta):\n",
    "    iters = 0\n",
    "    currPoint = x0\n",
    "    f_val,gradient,Q_x = readPoint(19598,x0)\n",
    "    S_ST = np.identity(10,dtype=float)\n",
    "    S_ST[0,0] = beta\n",
    "    if flag == 1:\n",
    "        descent_dir  = -gradient\n",
    "    else:\n",
    "        descent_dir = -np.dot(S_ST,gradient)\n",
    "    while np.linalg.norm(gradient) > 0.01:\n",
    "        step_size = exactLineSearch(currPoint,flag,S_ST)\n",
    "        currPoint = currPoint + step_size*descent_dir\n",
    "        ##print(\"x%d=%s\" % (iters, currPoint))\n",
    "        f_val,gradient,Q_x = readPoint(SR, currPoint)\n",
    "        if flag ==1:\n",
    "            descent_dir = -gradient\n",
    "        else:\n",
    "            descent_dir = -np.dot(S_ST,gradient)\n",
    "        iters += 1\n",
    "    return iters,currPoint,f_val\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.set_printoptions(precision=3)\n",
    "    ## part f subpart 3\n",
    "    iters,currPoint,f_val = gradientDescent(19598,np.array([1,50,50,50,50,50,50,50,50,50]),1,-1)\n",
    "    print('iters = %d , function value = %s for subpart 3'%(iters,f_val))\n",
    "    ## part f subpart 4\n",
    "    iters1,f_val1,currPoint1 = gradientDescent(19598,np.array([1,50,50,50,50,50,50,50,50,50]),2,1/200)\n",
    "    print('iters = %d , function value = %s,for beta = 1/200'%(iters1,f_val1))\n",
    "    iters2,f_val2,currPoint2 = gradientDescent(19598,np.array([1,50,50,50,50,50,50,50,50,50]),2,1/700)\n",
    "    print('iters = %d, function value = %s for beta = 1/700'%(iters2,f_val2))\n",
    "    iters3,f_val3,currPoint3 = gradientDescent(19598,np.array([1,50,50,50,50,50,50,50,50,50]),2,1/2000)\n",
    "    print('iters = %d function value = %s for beta = 1/2000'%(iters3,f_val3))\n",
    "    print('the best beta = 1/700 and corresponding function val is %s and the point to which it converges is'%(f_val2),end = ' ')\n",
    "    print(currPoint2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5fb7c4-3e93-4773-9e7e-7e52d9aa72e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eede8ce-36cc-4897-81ad-45792287bc63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e63454e-f5c9-44ba-ae1d-1848ebde8ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
